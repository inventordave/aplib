# APLIB C++ Wrapper provides an Interface, and associated domain-mappings, such as the intrinsic primitive C++ arithmetic operators can be overloaded to the C virtual operator function, and r-value/l-value type mappings can be made so that the user (a developer linking or embedding my library) can work seemlessly with arbitrary-precision arithmetic values as if the native C++ environment provided them intrinsically.

# A pre-processor invoked before the C++ pre-processor (simply speaking, before the invocation of the g++ compiler stack) could also add further featureness in transforming lexical literals in the source code that, at least in terms of valid C++ grammar, would not pass the static C++ syntax tree parsing phase without producing syntax error codes.

# "literal" could be a good name for the pre-processor. A makefile would then be generated by a tool, perhaps the same one, to generate make targets that invoke the "literal" pre-processor, then invoke g++. A pseudo-example might be:

program: source.cpp source.hpp aplib.cpp aplib.hpp
		literal.exe source \[ | source.hpp source.cpp ]
		g++ source.cpp aplib.cpp -laplib.lib -o program.exe
		
# If the makefile generated by the tool was not used, the developer's source files using C++ grammar-invalid literals for r-values, may fail to compile by the C++ compiler. If a msg could be generated by the compiler stack suggesting that the "literal" pre-processor should be applied, that would make the tool even more user-friendly. LLVM is an open-source compiler stack that supports C++ compilation out-of-the-box, but I'd also like to incorporate that support into a GCC compiler implementation. They're both fundamentally open-source, so if I am to produce modified versions of the compiler stacks, other tools might also have a place in these derivations.

Perhaps a compilation stack layer invoked after successful build of a compilation invocation, that after the codebase proves to succesfully compile, might run a "lint", or "pretty-print" pass over the source files. The prior-to-transformed source file strings could be preserved using the standard versioning system of git.

Outputting a printable/readable formatting of the types/objects/variables/functions and an analysis of control-flow and branch-predicting robustness of the source program architecture. Perhaps a prompt to transform the source machine (in sourcecode form) through a transpilation tool-chain to compatible target language systems (e.g., from C++ to Typescript).

Integration with a user-friendly, featureful IDE/Code Editor, and git-based versioning system that transparently integrates with the user's existing git-based tools (such as Github.com, etc).

I like Notepad++. It's also open-source. However, I have a desire to develop a grammar-highlighting text editor in a WebHTML/Gecko/etc browser-hosted window control. This would allow a plug-in API that supports javascript, and means intrinsic network/https support. Remote backup protocols would be easier to make available.

# RT-DT (ported to a native implementation) could use the aplib library for arbitrary-scale rendering. Then a suite of physics tools/libraries providing runtime-support for: Newtonian Mechanics, Einsteinian Relativity, a Kaluza-Klein toolset, a quantum mechanics toolset, which would be geared toward Feynmanian mapping, and statistical modelling, a "granular" engine (h-granules, and ST-discreteness under AB-Mechanics, which is my new, unfortunate, working name for Mopsian Mechanics)

# the "granular" engine would provide a base resolution/scale/precision for modelling ST volumes using RT-DT as the Visualiser. Adding time-based and keyframe-based animation, once correctly implemented, would provide arbitrarily-detailed rendering (subject to compute-resources) of physics models.

"granular"
"literal"
RT-DT

# A Doc tool based on XML/XSLT in the Framework would help in producing a unified System for building & deploying full end-to-end projects.

Some required industry-level standards: schema.org & ontological/semantic tagging, SGML-derived schema, GPU & SIMD protocol-standards, ABI-awareness is a big intellectual task as the more seamless the user-developer's ability to target their projects to any given target binary-architecture, the more MVP my development system would be to developers.

One front-end, one tech-stack, many customer platforms for the user-developer to deploy to.




# A subsytem for easily scripting or encoding time-based macros/automated tasks, such as ratio-based spread-betting on stock market indices, based on ticket curve-volatility, or mechanically-analyzed congressperson/publically-liable trade intelligence. It would be a fairly friendly configuration process of mapping a stock Ticket, with API/microservice-provided curve-point updates from a real-time data provider with a ratio-based decision heuristic that, based on safety limits, would auto- put or close stock trades

. A pseudo-example might be,

A table of the stocks a publically-liable investor (such as a Congress-person's publically-declared investments), with

A) The total amount they spent on all their stock purchases/sells in a declared period, and
B) The amount invested in each of the N stock-ticket investment (thus, the percentage % of total pot investment for each stock),

so that, if in pseudo-example, a publically-liable investor invested $100,000, and spent $15,000 on a named stock-Ticket, the investment tool might choose to spend 15% of the app user's pot on the ticket. If the app-user has an investible pot of $8,000, 15% of that (15,000/100,000) would be $1200. That way, the only way they can f\*ck you for taking advantage of their market knowledge would be to artificially reduce their trade/investment success. Any move they made to make the choices less profitable, would result in less profit for them.

A community-based pot-sharing scheme could potentially have an unpredictable side-effect of causing runs, should a sufficient number of app users invest in that community-pot, thus it's stock buying/selling decisions.

A blockchain-based IP stakeholding/license-transfer system would allow intangibles creators (of tv shows, movies, etc) to operate independantly of corporate market & product-space distribution mechanisms, such as networks and tv/movie studios & distributors. Then of course, an open-source media-streamng management suite that allowed efficient purchase/access licensing to media customers (people who watch tv shows & movies on streaming platforms), would certainly wrestle some of the monopolistic control currently held by Big Streaming, such as Netflix, Amazon, and so forth.


A magazine that provides thoughtful debate on issues that aren't really being reasoned about sensibly. What if the bombs drop? What if the power networks get destroyed, and there is a systemic shutdown of current systemic distribution systems? Were the Nazis right-wing? Or were they in fact left-wing, and the Communists engaged in a dialectic strategy to label them "right-wing"? Were the Nazis actually left-wing? What is the difference between someone being able to live their truth, and someone else as a result, not being able to?








3.14159...
